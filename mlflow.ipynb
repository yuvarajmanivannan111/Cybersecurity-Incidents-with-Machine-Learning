{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix, accuracy_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flow= pd.read_csv('GUIDE_Train.csv',nrows=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flow.columns = df_flow.columns.str.lower()\n",
    "\n",
    "# more then 50% null data columns\n",
    "df_flow = df_flow.drop(columns=['resourcetype',\n",
    " 'actiongrouped',\n",
    " 'actiongranular',\n",
    " 'threatfamily',\n",
    " 'emailclusterid',\n",
    " 'antispamdirection',\n",
    " 'roles',\n",
    " 'suspicionlevel',\n",
    " 'lastverdict',\n",
    " 'mitretechniques'])\n",
    "\n",
    "# dorp null rows overall data set\n",
    "df_flow = df_flow.dropna()\n",
    "\n",
    "# datetime Feature extraction\n",
    "df_flow['timestamp'] = pd.to_datetime(df_flow['timestamp'])\n",
    "\n",
    "df_flow['year'] = df_flow['timestamp'].dt.year\n",
    "df_flow['month'] = df_flow['timestamp'].dt.month\n",
    "df_flow['day'] = df_flow['timestamp'].dt.day\n",
    "df_flow['hour'] = df_flow['timestamp'].dt.hour\n",
    "df_flow['minute'] = df_flow['timestamp'].dt.minute\n",
    "\n",
    "df_flow.drop('timestamp', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "highly correlated features with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_corr_columns = ['accountobjectid','accountname','applicationname','sha256','folderpath','osversion','city','countrycode']\n",
    "df_flow = df_flow.drop(columns=hi_corr_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'orgid', 'incidentid', 'alertid', 'detectorid', 'alerttitle',\n",
       "       'category', 'incidentgrade', 'entitytype', 'evidencerole', 'deviceid',\n",
       "       'ipaddress', 'url', 'accountsid', 'accountupn', 'devicename',\n",
       "       'networkmessageid', 'registrykey', 'registryvaluename',\n",
       "       'registryvaluedata', 'applicationid', 'oauthapplicationid', 'filename',\n",
       "       'resourceidname', 'osfamily', 'state', 'year', 'month', 'day', 'hour',\n",
       "       'minute'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flow.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the target column to last index\n",
    "clumn = ['id', 'orgid', 'incidentid', 'alertid', 'detectorid', 'alerttitle',\n",
    "       'category', 'entitytype', 'evidencerole', 'deviceid',\n",
    "       'ipaddress', 'url', 'accountsid', 'accountupn', 'devicename',\n",
    "       'networkmessageid', 'registrykey', 'registryvaluename',\n",
    "       'registryvaluedata', 'applicationid', 'oauthapplicationid', 'filename',\n",
    "       'resourceidname', 'osfamily', 'state', 'year', 'month', 'day', 'hour',\n",
    "       'minute', 'incidentgrade']\n",
    "\n",
    "df_flow = df_flow[clumn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 99475 entries, 0 to 99999\n",
      "Data columns (total 31 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  99475 non-null  int64 \n",
      " 1   orgid               99475 non-null  int64 \n",
      " 2   incidentid          99475 non-null  int64 \n",
      " 3   alertid             99475 non-null  int64 \n",
      " 4   detectorid          99475 non-null  int64 \n",
      " 5   alerttitle          99475 non-null  int64 \n",
      " 6   category            99475 non-null  object\n",
      " 7   entitytype          99475 non-null  object\n",
      " 8   evidencerole        99475 non-null  object\n",
      " 9   deviceid            99475 non-null  int64 \n",
      " 10  ipaddress           99475 non-null  int64 \n",
      " 11  url                 99475 non-null  int64 \n",
      " 12  accountsid          99475 non-null  int64 \n",
      " 13  accountupn          99475 non-null  int64 \n",
      " 14  devicename          99475 non-null  int64 \n",
      " 15  networkmessageid    99475 non-null  int64 \n",
      " 16  registrykey         99475 non-null  int64 \n",
      " 17  registryvaluename   99475 non-null  int64 \n",
      " 18  registryvaluedata   99475 non-null  int64 \n",
      " 19  applicationid       99475 non-null  int64 \n",
      " 20  oauthapplicationid  99475 non-null  int64 \n",
      " 21  filename            99475 non-null  int64 \n",
      " 22  resourceidname      99475 non-null  int64 \n",
      " 23  osfamily            99475 non-null  int64 \n",
      " 24  state               99475 non-null  int64 \n",
      " 25  year                99475 non-null  int32 \n",
      " 26  month               99475 non-null  int32 \n",
      " 27  day                 99475 non-null  int32 \n",
      " 28  hour                99475 non-null  int32 \n",
      " 29  minute              99475 non-null  int32 \n",
      " 30  incidentgrade       99475 non-null  object\n",
      "dtypes: int32(5), int64(22), object(4)\n",
      "memory usage: 22.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_flow.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "incidentgrade\n",
       "BenignPositive    43024\n",
       "TruePositive      34887\n",
       "FalsePositive     21564\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flow['incidentgrade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incidentgrade\n",
      "BenignPositive    21564\n",
      "TruePositive      21564\n",
      "FalsePositive     21564\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Assuming your dataframe is called 'df' and the column is 'incidentgrade'\n",
    "\n",
    "# Get the value counts\n",
    "value_counts = df_flow['incidentgrade'].value_counts()\n",
    "\n",
    "# Find the minimum count\n",
    "min_count = value_counts.min()\n",
    "\n",
    "# Create a list to store the balanced dataframes\n",
    "balanced_dfs = []\n",
    "\n",
    "# Undersample each class\n",
    "for class_value in value_counts.index:\n",
    "    class_df = df_flow[df_flow['incidentgrade'] == class_value]\n",
    "\n",
    "    if len(class_df) > min_count:\n",
    "        # Undersample\n",
    "        undersampled_df = resample(class_df,\n",
    "                                   replace=False,    # sample without replacement\n",
    "                                   n_samples=min_count,\n",
    "                                   random_state=42)  # reproducible results\n",
    "        balanced_dfs.append(undersampled_df)\n",
    "    else:\n",
    "        # If this class is already at or below the minimum, keep all samples\n",
    "        balanced_dfs.append(class_df)\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced = pd.concat(balanced_dfs)\n",
    "\n",
    "# Verify the new class distribution\n",
    "print(df_balanced['incidentgrade'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_column = df_balanced.select_dtypes(include='object').columns\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in categorical_column:\n",
    "    df_balanced[col] = le.fit_transform(df_balanced[col])\n",
    "\n",
    "X = df_balanced.iloc[:,:-1]\n",
    "y = df_balanced.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\New\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.60      0.48      4313\n",
      "           1       0.43      0.23      0.30      4313\n",
      "           2       0.51      0.50      0.51      4313\n",
      "\n",
      "    accuracy                           0.44     12939\n",
      "   macro avg       0.45      0.44      0.43     12939\n",
      "weighted avg       0.45      0.44      0.43     12939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\New\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters in a dictionary\n",
    "params = {\n",
    "    'random_state': None,\n",
    "    'multi_class': 'auto',\n",
    "    'solver': 'lbfgs',\n",
    "    'max_iter': 100\n",
    "}\n",
    "# Initialize the Logistic Regression model with the defined parameters\n",
    "model = LogisticRegression(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "report_dict = classification_report(y_test,y_pred)\n",
    "print(report_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87      4313\n",
      "           1       0.90      0.89      0.89      4313\n",
      "           2       0.93      0.87      0.90      4313\n",
      "\n",
      "    accuracy                           0.89     12939\n",
      "   macro avg       0.89      0.89      0.89     12939\n",
      "weighted avg       0.89      0.89      0.89     12939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters for Random Forest in a dictionary\n",
    "rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'random_state': None,\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'bootstrap': True\n",
    "}\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred_rf)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.43      0.43      4313\n",
      "           1       0.33      0.28      0.31      4313\n",
      "           2       0.42      0.47      0.44      4313\n",
      "\n",
      "    accuracy                           0.40     12939\n",
      "   macro avg       0.39      0.40      0.39     12939\n",
      "weighted avg       0.39      0.40      0.39     12939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters for KNN in a dictionary\n",
    "knn_params = {\n",
    "    'n_neighbors': 2,  # Number of neighbors to use\n",
    "    'weights': 'uniform',  # Weight function used in prediction\n",
    "    'algorithm': 'auto',  # Algorithm used to compute the nearest neighbors\n",
    "    'p': 2  # Power parameter for the Minkowski metric (2 for Euclidean distance)\n",
    "}\n",
    "\n",
    "# Initialize the KNN model with the defined parameters\n",
    "knn_model = KNeighborsClassifier(**knn_params)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "knn_report = classification_report(y_test,y_pred)\n",
    "print(knn_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\New\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:01:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.60      0.48      4313\n",
      "           1       0.43      0.23      0.30      4313\n",
      "           2       0.51      0.50      0.51      4313\n",
      "\n",
      "    accuracy                           0.44     12939\n",
      "   macro avg       0.45      0.44      0.43     12939\n",
      "weighted avg       0.45      0.44      0.43     12939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': 100,  # Number of boosting rounds\n",
    "    'max_depth': 3,  # Maximum depth of trees\n",
    "    'learning_rate': 0.1,  # Step size shrinkage\n",
    "    'subsample': 0.8,  # Proportion of samples to use for each tree\n",
    "    'colsample_bytree': 0.8,  # Proportion of features to use for each tree\n",
    "    'objective': 'multi:softmax',  # Multi-class classification objective\n",
    "    'num_class': 3,  # Number of classes\n",
    "    'eval_metric': 'mlogloss'  # Evaluation metric\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(**xgb_params, use_label_encoder=False)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "xgb_report = classification_report(y_test,y_pred)\n",
    "print(xgb_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.60      0.48      4313\n",
      "           1       0.43      0.23      0.30      4313\n",
      "           2       0.51      0.50      0.51      4313\n",
      "\n",
      "    accuracy                           0.44     12939\n",
      "   macro avg       0.45      0.44      0.43     12939\n",
      "weighted avg       0.45      0.44      0.43     12939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the parameters for Gradient Boosting in a dictionary\n",
    "gb_params = {\n",
    "    'n_estimators': 100,  # Number of boosting stages to be run\n",
    "    'learning_rate': 0.1,  # Learning rate shrinks the contribution of each tree\n",
    "    'max_depth': 3,  # Maximum depth of the individual trees\n",
    "    'subsample': 1.0,  # Proportion of samples to use for fitting each individual tree\n",
    "    'criterion': 'friedman_mse'  # Function to measure the quality of a split (default is 'friedman_mse')\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingClassifier(**gb_params)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "gb_report = classification_report(y_test,y_pred)\n",
    "print(gb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'random_state': None,\n",
    "    'multi_class': 'auto',\n",
    "    'solver': 'lbfgs',\n",
    "    'max_iter': 100\n",
    "}\n",
    "rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'random_state': None,\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'bootstrap': True\n",
    "}\n",
    "knn_params = {\n",
    "    'n_neighbors': 2,  # Number of neighbors to use\n",
    "    'weights': 'uniform',  # Weight function used in prediction\n",
    "    'algorithm': 'auto',  # Algorithm used to compute the nearest neighbors\n",
    "    'p': 2  # Power parameter for the Minkowski metric (2 for Euclidean distance)\n",
    "}\n",
    "xgb_params = {\n",
    "    'n_estimators': 100,  # Number of boosting rounds\n",
    "    'max_depth': 3,  # Maximum depth of trees\n",
    "    'learning_rate': 0.1,  # Step size shrinkage\n",
    "    'subsample': 0.8,  # Proportion of samples to use for each tree\n",
    "    'colsample_bytree': 0.8,  # Proportion of features to use for each tree\n",
    "    'objective': 'multi:softmax',  # Multi-class classification objective\n",
    "    'num_class': 3,  # Number of classes\n",
    "    'eval_metric': 'mlogloss'  # Evaluation metric\n",
    "}\n",
    "gb_params = {\n",
    "    'n_estimators': 100,  # Number of boosting stages to be run\n",
    "    'learning_rate': 0.1,  # Learning rate shrinks the contribution of each tree\n",
    "    'max_depth': 3,  # Maximum depth of the individual trees\n",
    "    'subsample': 1.0,  # Proportion of samples to use for fitting each individual tree\n",
    "    'criterion': 'friedman_mse'  # Function to measure the quality of a split (default is 'friedman_mse')\n",
    "}\n",
    "models = [(\n",
    "    \"Logistic regression\",\n",
    "    LogisticRegression(**lr_params),\n",
    "    (X_train,y_train),\n",
    "    (X_test,y_test)\n",
    "),(\n",
    "    \"Random Forest\",\n",
    "    RandomForestClassifier(**rf_params),\n",
    "    (X_train,y_train),\n",
    "    (X_test,y_test)\n",
    "),(\n",
    "    \"KNN\",\n",
    "    KNeighborsClassifier(**knn_params),\n",
    "    (X_train,y_train),\n",
    "    (X_test,y_test)\n",
    "),(\n",
    "    \"XGBClassifier\",\n",
    "    XGBClassifier(**xgb_params),\n",
    "    (X_train,y_train),\n",
    "    (X_test,y_test)\n",
    "),(\n",
    "    \"GradientBoostingClassifier\",\n",
    "    GradientBoostingClassifier(**gb_params),\n",
    "    (X_train,y_train),\n",
    "    (X_test,y_test)\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\New\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\New\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "for model_name,model,train_set,test_set in models:\n",
    "    X_train = train_set[0]\n",
    "    y_train = train_set[1]\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test,y_pred,output_dict=True)\n",
    "    reports.append(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiple model log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/04 13:04:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/09/04 13:04:22 INFO mlflow.tracking._tracking_service.client: 🏃 View run Logistic regression at: http://127.0.0.1:5000/#/experiments/435085711337647451/runs/b63a4e1e4f964883b9bd28fb344c2cff.\n",
      "2024/09/04 13:04:22 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/435085711337647451.\n",
      "2024/09/04 13:04:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/09/04 13:04:27 INFO mlflow.tracking._tracking_service.client: 🏃 View run Random Forest at: http://127.0.0.1:5000/#/experiments/435085711337647451/runs/57425fd6b9e94289b99f7520e5bbfac0.\n",
      "2024/09/04 13:04:27 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/435085711337647451.\n",
      "2024/09/04 13:04:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/09/04 13:04:31 INFO mlflow.tracking._tracking_service.client: 🏃 View run KNN at: http://127.0.0.1:5000/#/experiments/435085711337647451/runs/fb37e585ef09474b9a0262340b8fe799.\n",
      "2024/09/04 13:04:31 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/435085711337647451.\n",
      "c:\\Users\\New\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:04:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "2024/09/04 13:04:35 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/09/04 13:04:35 INFO mlflow.tracking._tracking_service.client: 🏃 View run XGBClassifier at: http://127.0.0.1:5000/#/experiments/435085711337647451/runs/be70defe3fd44f6184785cc042a5dabd.\n",
      "2024/09/04 13:04:35 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/435085711337647451.\n",
      "2024/09/04 13:04:39 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/09/04 13:04:39 INFO mlflow.tracking._tracking_service.client: 🏃 View run GradientBoostingClassifier at: http://127.0.0.1:5000/#/experiments/435085711337647451/runs/af9c0f70d1f44bbf94ec4cb715a97831.\n",
      "2024/09/04 13:04:39 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/435085711337647451.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"microsoft_classification\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "\n",
    "for i ,element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    model = element[1]\n",
    "    report = reports[i]\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        mlflow.log_param(\"model_name\",model_name)\n",
    "        mlflow.log_metric(\"accuracy\",report[\"accuracy\"])\n",
    "        mlflow.log_metric(\"recall_class_0\",report['0']['recall'])\n",
    "        mlflow.log_metric(\"recall_class_1\",report['1']['recall'])\n",
    "        mlflow.log_metric(\"recall_class_2\",report['2']['recall'])\n",
    "        mlflow.log_metric('f1_score_macro', report['macro avg']['f1-score'])\n",
    "\n",
    "        if \"XGB\" in model_name:\n",
    "            mlflow.xgboost.log_model(model, model_name)\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, model_name)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single model log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/02 17:04:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/09/02 17:04:02 INFO mlflow.tracking._tracking_service.client: 🏃 View run rare-kit-813 at: http://127.0.0.1:5000/#/experiments/344090298825934818/runs/9ace973b3e7e4bef870370bbced208e2.\n",
      "2024/09/02 17:04:02 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/344090298825934818.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"first Expriment\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\" : report_dict['accuracy'],\n",
    "        \"recall_class_0\" : report_dict['0']['recall'],\n",
    "        \"recall_class_1\" : report_dict['1']['recall'],\n",
    "        \"recall_class_2\" : report_dict['2']['recall'],\n",
    "        \"f1_score_macro\" : report_dict['macro avg']['f1-score']\n",
    "        \n",
    "    })\n",
    "    mlflow.sklearn.log_model(rf_model,\"Random forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Generate classification report\n",
    "best_rf_report = classification_report(y_test, y_pred_best_rf)\n",
    "\n",
    "print(\"Best Parameters from Grid Search:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\nClassification Report of Best Model:\")\n",
    "print(best_rf_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN k-value finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the data (if not already split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 31)\n",
    "\n",
    "# Store the mean error for each k value\n",
    "mean_errors = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Initialize the KNN model with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation data\n",
    "    y_pred = knn.predict(X_val)\n",
    "    \n",
    "    # Calculate the mean squared error\n",
    "    error = mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    # Store the error\n",
    "    mean_errors.append(error)\n",
    "\n",
    "# Find the k value with the minimum error\n",
    "best_k = k_values[np.argmin(mean_errors)]\n",
    "print(f\"Best k value: {best_k}\")\n",
    "\n",
    "# Plotting the error vs. k values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, mean_errors, marker='o')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Error vs. k Value')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
